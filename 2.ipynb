{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Agent, Crew, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key='AIzaSyBKo19PtvV9oSMRr4R1wJUueyWOL4n5e5c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_agent = Agent(\n",
    "    role='Senior content Writer',\n",
    "    goal='To write shot and sweet narration for YouTube short videos',\n",
    "    backstory=\"\"\"You are a content writer for a YouTube creator.\n",
    "    With a flair for simplifying complex topics, you craft engaging narratives that captivate and educate audience.\n",
    "    You are responsible for writing engaging narrations for the YouTube short videos.\n",
    "    Your narrations are concise and engaging and always grabs audience's attention.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation = False\n",
    ")\n",
    "\n",
    "image_descriptive_agent = Agent(\n",
    "    role='Visuals writer',\n",
    "    goal='To create stunning visuals for the YouTube short videos',\n",
    "    backstory='You are an expert in visual storytelling and creating compelling imagery.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation = False\n",
    ")\n",
    "\n",
    "img_generator_agent = Agent(\n",
    "    role='Media Content Creator',\n",
    "    goal='To generate images for YouTube short videos based on the provided narration',\n",
    "    backstory='You are an expert in creating multimedia content.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    tools=[image_generator]\n",
    ")\n",
    "\n",
    "\n",
    "story_writing_task = Task(\n",
    "    description='Write an engaging narration for a YouTube short video on the topic: {topic}',\n",
    "    expected_output=\"\"\"A short paragraph suitable for narrating in five seconds also provides immensive experice to audience. Folow the below example for output length and format.\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    **topic:**\n",
    "    Powerful Kings of History\n",
    "\n",
    "    **narration:**\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations.\n",
    "    From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.\n",
    "    Their leadership continues to inspire awe and fascination to this day.\n",
    "    \"\"\",\n",
    "    agent=script_agent\n",
    ")\n",
    "\n",
    "img_text_task = Task(\n",
    "    description='Given the narration, write text for each sentence in the narration that is  used as a prompt for an image generation.',\n",
    "    expected_output=\"\"\"Follow the below example for output format.\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    **narration:**\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations. From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations. Their leadership continues to inspire awe and fascination to this day.\n",
    "\n",
    "    **text descriptions:**\n",
    "    <narration>In the pages of history, powerful kings have shaped the destinies of nations.<narration>\n",
    "    <image>An epic portrayal of ancient kings standing triumphantly, clad in regal attire, commanding their kingdoms with strength and wisdom, amidst grandeur and splendor.<image>\n",
    "    <narration>From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.<narration>\n",
    "    <image>Dramatic portraits of Alexander the Great and Genghis Khan, adorned in battle armor, leading their armies across vast landscapes and leaving a lasting mark on history.<image>\n",
    "    <narration>Their leadership continues to inspire awe and fascination to this day.<narration>\n",
    "    <image>A powerful visual of kings seated on thrones, symbols of authority and ambition, evoking admiration and wonder, against a backdrop of their enduring achievements.<image>\n",
    "    \"\"\",\n",
    "    agent=image_descriptive_agent,\n",
    "    context=[story_writing_task]\n",
    ")\n",
    "\n",
    "img_generation_task = Task(\n",
    "    description='Pass the text enclosed in <image> tag into the used including the tags',\n",
    "    expected_output=\"\"\"path of the folder where images are saved\"\"\",\n",
    "    context = [img_text_task],\n",
    "    agent=img_generator_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "@tool\n",
    "def text_extrator(narration):\n",
    "    \"\"\"Extracts text used for image generation and speech generation from given narration\"\"\"\n",
    "    text_for_image_generation = re.findall(r'<image>(.*?)<image>', narration)\n",
    "    text_for_speech_generation = re.findall(r'<narration>(.*?)<narration>', narration)\n",
    "    return text_for_image_generation, text_for_speech_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d65ea7bd9d740ec9dcf653eaeed0198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionXLPipeline, DPMSolverSinglestepScheduler\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"sd-community/sdxl-flash\", torch_dtype=torch.float16).to('cuda')\n",
    "pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "def quantize_model_to_4bit(model):\n",
    "    replacements = []\n",
    "\n",
    "    # Collect layers to be replaced\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            replacements.append((name, module))\n",
    "\n",
    "    # Replace layers\n",
    "    for name, module in replacements:\n",
    "        # Split the name to navigate to the parent module\n",
    "        *path, last = name.split('.')\n",
    "        parent = model\n",
    "        for part in path:\n",
    "            parent = getattr(parent, part)\n",
    "\n",
    "        # Create and assign the quantized layer\n",
    "        quantized_layer = bnb.nn.Linear4bit(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "        quantized_layer.weight.data = module.weight.data\n",
    "        if module.bias is not None:\n",
    "            quantized_layer.bias.data = module.bias.data\n",
    "        setattr(parent, last, quantized_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "pipe.unet = quantize_model_to_4bit(pipe.unet)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import os\n",
    "\n",
    "def generate_speech(text, lang='en', speed=170, voice='default', num=0):\n",
    "    \"\"\"\n",
    "    Generates speech for given script.\n",
    "    \"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Set language and voice\n",
    "    voices = engine.getProperty('voices')\n",
    "    if voice == 'default':\n",
    "        voice_id = voices[0].id\n",
    "    else:\n",
    "        # Try to find the voice with the given name\n",
    "        voice_id = None\n",
    "        for v in voices:\n",
    "            if voice in v.name:\n",
    "                voice_id = v.id\n",
    "                break\n",
    "        if not voice_id:\n",
    "            raise ValueError(f\"Voice '{voice}' not found.\")\n",
    "    \n",
    "    engine.setProperty('voice', voice_id)\n",
    "    engine.setProperty('rate', speed)\n",
    "    print(os.path.join(os.path.dirname(os.path.abspath(__file__)), f'outputs/speech/speech_{num}.mp3'))\n",
    "    engine.save_to_file(text, os.path.join(os.path.dirname(os.path.abspath(__file__)), f'outputs/speech/speech_{num}.mp3'))\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tool\n",
    "def image_generator(narration, images_dir='./outputs/images.jpg'):\n",
    "    \"\"\"Generates images for the given narration.\n",
    "    Saves it to images_dir and return path\"\"\"\n",
    "    image = pipe(narration, num_inference_steps=6, guidance_scale=2, width=720, height=1280, verbose=0).images[0]\n",
    "    image.save('outputs/image.png')#os.path.join(images_dir, f'image.jpg'))\n",
    "    return f'image generated for {narration} and saved to directory {images_dir}'\n",
    "\n",
    "# @tool\n",
    "def speech_generator(text, speech_dir='./outputs/audio'):\n",
    "    \"\"\"Generates speech for given text\"\"\"\n",
    "    generate_speech(text, speech_dir, num=0)\n",
    "    return f'speech generated for {text} and saved to directory {speech_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:58:42,213 - 12656 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[script_agent, image_descriptive_agent, img_generator_agent],\n",
    "    tasks=[story_writing_task, img_text_task, img_generation_task],\n",
    "    process = Process.sequential,\n",
    "    # cache = True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crew.kickoff(inputs={'topic': 'Democracy in India'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9761ac05484c2f80e5ce3d374ab395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mimage_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m, in \u001b[0;36mimage_generator\u001b[1;34m(narration, images_dir)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates images for the given narration.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mSaves it to images_dir and return path\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m pipe(narration, num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, guidance_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m720\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs/image.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#os.path.join(images_dir, f'image.jpg'))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage generated for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnarration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and saved to directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\PIL\\Image.py:2429\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2427\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2428\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2429\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2432\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/image.png'"
     ]
    }
   ],
   "source": [
    "image_generator('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('C:\\\\Users\\\\prudh\\\\Desktop\\\\Python\\\\Generative AI\\\\GenerativeAI-Projects\\\\10sec Shorts\\\\outpus\\\\images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'<image>(.*?)<image>', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'<narration>(.*?)<narration>', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'<speech>(.*?)</speech>', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
