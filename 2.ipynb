{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Agent, Crew, Process\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key='AIzaSyBKo19PtvV9oSMRr4R1wJUueyWOL4n5e5c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model='llama3-70b-8192', temperature=1.0, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "# llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a large language model trained on a massive dataset of text from the internet, which allows me to generate human-like responses to a wide range of topics and questions.\\n\\nI'm not a human, but I'm designed to simulate conversation in a way that feels natural and intuitive. I can understand and respond to questions, provide information, tell jokes, and even create stories or poems. I'm constantly learning and improving my responses based on the interactions I have with users like you.\\n\\nI'm here to help, inform, and entertain you. So, how can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 13, 'total_tokens': 158, 'completion_time': 0.414285714, 'prompt_time': 0.004681102, 'queue_time': None, 'total_time': 0.418966816}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a4a74f0f-6f2a-4b76-8f16-d6eb3d1fe145-0', usage_metadata={'input_tokens': 13, 'output_tokens': 145, 'total_tokens': 158})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('who are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f976b35fb048d19ebfa8428e565281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionXLPipeline, DPMSolverSinglestepScheduler\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"sd-community/sdxl-flash\", torch_dtype=torch.float16).to('cuda')\n",
    "pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "def quantize_model_to_4bit(model):\n",
    "    replacements = []\n",
    "\n",
    "    # Collect layers to be replaced\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            replacements.append((name, module))\n",
    "\n",
    "    # Replace layers\n",
    "    for name, module in replacements:\n",
    "        # Split the name to navigate to the parent module\n",
    "        *path, last = name.split('.')\n",
    "        parent = model\n",
    "        for part in path:\n",
    "            parent = getattr(parent, part)\n",
    "\n",
    "        # Create and assign the quantized layer\n",
    "        quantized_layer = bnb.nn.Linear4bit(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "        quantized_layer.weight.data = module.weight.data\n",
    "        if module.bias is not None:\n",
    "            quantized_layer.bias.data = module.bias.data\n",
    "        setattr(parent, last, quantized_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "pipe.unet = quantize_model_to_4bit(pipe.unet)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import os\n",
    "\n",
    "def generate_speech(text, speech_dir='./outputs/audio', lang='en', speed=170, voice='default', num=0):\n",
    "    \"\"\"\n",
    "    Generates speech for given script.\n",
    "    \"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Set language and voice\n",
    "    voices = engine.getProperty('voices')\n",
    "    if voice == 'default':\n",
    "        voice_id = voices[1].id\n",
    "    else:\n",
    "        # Try to find the voice with the given name\n",
    "        voice_id = None\n",
    "        for v in voices:\n",
    "            if voice in v.name:\n",
    "                voice_id = v.id\n",
    "                break\n",
    "        if not voice_id:\n",
    "            raise ValueError(f\"Voice '{voice}' not found.\")\n",
    "    \n",
    "    engine.setProperty('voice', voice_id)\n",
    "    engine.setProperty('rate', speed)\n",
    "    os.remove(os.path.join(speech_dir, f'speech_{num}.mp3')) if os.path.exists(os.path.join(speech_dir, f'speech_{num}.mp3')) else None\n",
    "    engine.save_to_file(text, os.path.join(speech_dir, f'speech_{num}.mp3'))\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "class ImageGeneration(BaseModel):\n",
    "    text : str = Field(description='description of sentence used for image generation')\n",
    "    num : int = Field(description='sequence of description passed this tool. Used in image saving path.')\n",
    "\n",
    "class SpeechGeneration(BaseModel):\n",
    "    text : str = Field(description='description of sentence used for image generation')\n",
    "    num : int = Field(description='sequence of description passed this tool. Used in image saving path.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=ImageGeneration)\n",
    "def image_generator(text,num):\n",
    "    \"\"\"Generates images for the given narration.\n",
    "    Saves it to images_dir and return path\"\"\"\n",
    "    images_dir = './outputs/images'\n",
    "    if '<image>' in text:\n",
    "        text = re.findall(r'<image>(.*?)<image>', text)\n",
    "    else:\n",
    "        text = text\n",
    "    image = pipe(text, num_inference_steps=6, guidance_scale=2, width=720, height=1280, verbose=0).images[0]\n",
    "    # print(num)\n",
    "    os.remove(os.path.join(images_dir, f'image{num}.jpg')) if os.path.exists(os.path.join(images_dir, f'image{num}.jpg')) else None\n",
    "    image.save(os.path.join(images_dir, f'image{num}.jpg'))\n",
    "    return f'image generated for \"{text}\" and saved to directory {images_dir} as image{num}.jpg'\n",
    "\n",
    "@tool\n",
    "def speech_generator(text, num):\n",
    "    \"\"\"Generates speech for given text\n",
    "    Saves it to speech_dir and return path\"\"\"\n",
    "    speech_dir = './outputs/audio'\n",
    "    if '<narration>' in text:\n",
    "        text = re.findall(r'<narration>(.*?)<narration>', text)\n",
    "    else:\n",
    "        text = text\n",
    "    # print(num)\n",
    "    generate_speech(text, speech_dir, num=num)\n",
    "    return f'speech generated for \"{text}\" and saved to directory {speech_dir} as speech{num}.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCreator(BaseModel):\n",
    "    images_dir : str = Field(description='Path to access images folder')\n",
    "    audios_dir : str = Field(description='Path to access audios folder')\n",
    "\n",
    "class ConcatenateVideos(BaseModel):\n",
    "    videos_dir : str = Field(description='Path to access small videos folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "@tool(args_schema=VideoCreator)\n",
    "def video_create(images_dir, audios_dir):\n",
    "    \"\"\"Given images directory and audios directory, combines images and audios to make a video.\n",
    "    Returns the path of the output directory.\"\"\"\n",
    "\n",
    "    output_dir = './outputs/temp_videos'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    images = os.listdir(images_dir)\n",
    "    speeches = os.listdir(audios_dir)\n",
    "    print('h|')\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "            output_path = os.path.join(output_dir, f'video_{i}.mp4')\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "\n",
    "            # Construct full paths to image and audio files\n",
    "            image_path = os.path.join(images_dir, images[i])\n",
    "            audio_path = os.path.join(audios_dir, speeches[i])\n",
    "            print(image_path)\n",
    "            print(audio_path)\n",
    "\n",
    "            # ffmpeg command to create video\n",
    "            ffmpeg_command = [\n",
    "                'ffmpeg',\n",
    "                '-loop', '1',\n",
    "                '-i', image_path,\n",
    "                '-i', audio_path,\n",
    "                '-c:v', 'libx264',\n",
    "                '-c:a', 'aac',\n",
    "                '-b:a', '192k',\n",
    "                '-shortest',\n",
    "                output_path\n",
    "            ]\n",
    "\n",
    "            # Run ffmpeg command\n",
    "            print('hi')\n",
    "            result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "            print('bye')\n",
    "            # Optionally print ffmpeg output\n",
    "            # print(result.stdout)\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"FFmpeg command failed with error code {e.returncode}: {e.stderr}\")\n",
    "\n",
    "    return f'small videos are created using images and speeches and stored in {output_dir}'\n",
    "\n",
    "@tool(args_schema=ConcatenateVideos)\n",
    "def concatenate_videos(video_dir):\n",
    "    \"\"\"Concatenates small videos to make a complete video\"\"\"\n",
    "    video_list = [os.path.join(video_dir, video) for video in os.listdir(video_dir) if video.endswith('.mp4')]\n",
    "    output_dir = './outputs/final_video'\n",
    "    os.remove(os.path.join(output_dir, 'video.mp4')) if os.path.exists(os.path.join(output_dir, 'video.mp4')) else None\n",
    "    try:\n",
    "        inputs = []\n",
    "        filter_complex = ''\n",
    "        \n",
    "        for i, video in enumerate(video_list):\n",
    "            inputs.extend(['-i', video])\n",
    "            filter_complex += f'[{i}:v:0] [{i}:a:0] '\n",
    "        \n",
    "        filter_complex += f'concat=n={len(video_list)}:v=1:a=1 [v] [a]'\n",
    "        \n",
    "        ffmpeg_command = [\n",
    "            'ffmpeg',\n",
    "            *inputs,\n",
    "            '-filter_complex', filter_complex,\n",
    "            '-map', '[v]',\n",
    "            '-map', '[a]',\n",
    "            os.path.join(output_dir, 'video.mp4')\n",
    "        ]\n",
    "        \n",
    "        subprocess.run(ffmpeg_command, check=True, text=True)\n",
    "        # print(f\"Videos {video_list} have been successfully concatenated to './outputs/final_video/video.mp4'.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"FFmpeg command failed with error code {e.returncode}:\")\n",
    "        print(e.stderr)\n",
    "\n",
    "    return f'full video created using small videos and stored in {output_dir} as video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_agent = Agent(\n",
    "    role='Senior Content Writer',\n",
    "    goal='Craft engaging, concise, and informative narrations for YouTube short videos',\n",
    "    backstory=\"\"\"As a seasoned content writer, you excel at breaking down complex topics into captivating narratives that educate and entertain audiences. Your expertise lies in writing concise, attention-grabbing scripts for YouTube short videos.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "image_descriptive_agent = Agent(\n",
    "    role='Visual Storyteller',\n",
    "    goal='Design stunning, contextually relevant visuals for YouTube short videos',\n",
    "    backstory='With a keen eye for visual storytelling, you create compelling imagery that elevates the narrative and captivates the audience. You will ',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "img_speech_generating_agent = Agent(\n",
    "    role='Multimedia Content Creator',\n",
    "    goal='Generate high-quality images and speech for YouTube short videos based on provided narrations',\n",
    "    backstory='As a multimedia expert, you excel at creating engaging multimedia content that brings stories to life.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "# editor = Agent(\n",
    "#     role='Video editor',\n",
    "#     goal='To make a full video for YouTube shorts.',\n",
    "#     backstory=\"\"\"You are a video editor working for a YouTube creator.\n",
    "#     You are expert in making small videos by using only images and audios\n",
    "#     Then you wil combine these small videos to make full video\"\"\",\n",
    "#     verbose=True,\n",
    "#     llm=llm,\n",
    "#     allow_delegation = False,\n",
    "#     tools = [video_create, concatenate_videos]\n",
    "# )\n",
    "\n",
    "editor = Agent(\n",
    "    role = 'Video editor',\n",
    "    goal = 'To make a video for YouTube shorts.',\n",
    "    backstory = \"You are a video editor working for a YouTube creator\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation = False,\n",
    "    tools = [create_video_from_images_and_audio]\n",
    ")\n",
    "\n",
    "story_writing_task = Task(\n",
    "    description='Write an engaging narration for a YouTube short video on the topic: {topic}',\n",
    "    expected_output=\"\"\"A short paragraph suitable for narrating in five seconds also provides immensive experice to audience. Folow the below example for output length and format.\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    **topic:**\n",
    "    Powerful Kings of History\n",
    "\n",
    "    **narration:**\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations.\n",
    "    From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.\n",
    "    Their leadership continues to inspire awe and fascination to this day.\n",
    "    \"\"\",\n",
    "    agent=script_agent\n",
    ")\n",
    "\n",
    "img_text_task = Task(\n",
    "    description='Given the narration,visually describe each sentence in the narration which will be used as a prompt for an image generation.',\n",
    "    expected_output=\"\"\"Sentences encoded in <narration> and <image> tags. Follow the below example for output format.\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    **narration:**\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations. From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations. Their leadership continues to inspire awe and fascination to this day.\n",
    "\n",
    "    **text descriptions:**\n",
    "    <narration>In the pages of history, powerful kings have shaped the destinies of nations.<narration>\n",
    "    <image>An epic portrayal of ancient kings standing triumphantly, clad in regal attire, commanding their kingdoms with strength and wisdom, amidst grandeur and splendor.<image>\n",
    "    <narration>From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.<narration>\n",
    "    <image>Dramatic portraits of Alexander the Great and Genghis Khan, adorned in battle armor, leading their armies across vast landscapes and leaving a lasting mark on history.<image>\n",
    "    <narration>Their leadership continues to inspire awe and fascination to this day.<narration>\n",
    "    <image>A powerful visual of kings seated on thrones, symbols of authority and ambition, evoking admiration and wonder, against a backdrop of their enduring achievements.<image>\n",
    "    \"\"\",\n",
    "    agent=image_descriptive_agent,\n",
    "    context=[story_writing_task]\n",
    ")\n",
    "\n",
    "img_generation_task = Task(\n",
    "    description='Given the input generate images for each sentence enclosed in <image> tag.',\n",
    "    expected_output=\"\"\"Acknowledgement of image generation\"\"\",\n",
    "    tools = [image_generator],\n",
    "    context = [img_text_task],\n",
    "    # async_execution=True,\n",
    "    agent=img_speech_generating_agent\n",
    ")\n",
    "\n",
    "speech_generation_task = Task(\n",
    "    description='Given the input generate speech for each sentence enclosed in <narration> tag.',\n",
    "    expected_output=\"\"\"Acknowledgement of speech generation\"\"\",\n",
    "    tools = [speech_generator],\n",
    "    context = [img_text_task],\n",
    "    # async_execution=True,\n",
    "    agent=img_speech_generating_agent\n",
    ")\n",
    "\n",
    "# video_create_task = Task(\n",
    "#     description='Create a video using images and audio from the folder \"outpus/images\" and \"outputs/audio\"',\n",
    "#     expected_output=\"\"\"Acknowledgement of small videos generation\"\"\",\n",
    "#     agent=editor,\n",
    "#     context = [img_generation_task, speech_generation_task],\n",
    "    \n",
    "# )\n",
    "\n",
    "# final_video_create_task = Task(\n",
    "#     description = 'create a full video using the small videos from the folder \"outputs/temp_videos\"',\n",
    "#     expected_output = \"\"\"Acknowledgement of full video generation\"\"\",\n",
    "#     context = [video_create_task],\n",
    "#     agent = editor,\n",
    "# )\n",
    "\n",
    "make_video_task = Task(\n",
    "    description = 'Create video using images and audios from the forlders \"outpus/images\" and \"outputs/audio\"',\n",
    "    expected_output = \"output video path\",\n",
    "    agent=editor,\n",
    "    context = [img_generation_task, speech_generation_task]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 11:22:26,043 - 13144 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[script_agent, image_descriptive_agent, img_speech_generating_agent, editor],\n",
    "    tasks=[story_writing_task, img_text_task, img_generation_task,speech_generation_task,make_video_task],\n",
    "    process = Process.sequential,\n",
    "    cache = True,\n",
    "    # memory=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crew.kickoff(inputs={'topic': 'Abilities of Lion'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full video created using small videos and stored in ./outputs/final_video as video.mp4'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
    "@tool\n",
    "def create_video_from_images_and_audio(image_dir, audio_dir):\n",
    "    \"\"\"Creates video using images and audios\"\"\"\n",
    "    images_paths = os.listdir(image_dir)\n",
    "    audio_paths = os.listdir(audio_dir)\n",
    "    print(images_paths, audio_paths)\n",
    "    clips = []\n",
    "    \n",
    "    for i in range(min(len(image_paths), len(audio_paths))):\n",
    "        # Load the image\n",
    "        img_clip = ImageClip(os.path.join(image_dir, images_paths[i]))\n",
    "        \n",
    "        # Load the audio file\n",
    "        audioclip = AudioFileClip(os.path.join(audio_dir, audio_paths[i]))\n",
    "        \n",
    "        # Set the duration of the video clip to the duration of the audio file\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        \n",
    "        # Add audio to the video clip\n",
    "        videoclip = videoclip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(videoclip)\n",
    "    \n",
    "    # Concatenate all video clips\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    \n",
    "    # Write the result to a file\n",
    "    final_clip.write_videofile(\"final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    return \"final_video.mp4\"\n",
    "\n",
    "# Example usage\n",
    "# image_paths = \"outputs/images\"\n",
    "# audio_paths = \"outputs/audio\"\n",
    "\n",
    "# video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "# print(f\"Video created at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('./outputs/audio\\speech_1.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_create(images_dir, audios_dir):\n",
    "    \"\"\"Given images directory and audios directory, combines images and audios to make a video.\n",
    "    Returns the path of the output directory.\"\"\"\n",
    "\n",
    "    output_dir = './outputs/temp_videos'\n",
    "    images = os.listdir(images_dir)\n",
    "    speeches = os.listdir(audios_dir)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "            os.remove(os.path.join(output_dir, f'video_{i}.mp4')) if os.path.exists(os.path.join(output_dir, f'video_{i}.mp4')) else None\n",
    "\n",
    "            # Construct full paths to image and audio files\n",
    "            image_path = os.path.join(images_dir, images[i])\n",
    "            audio_path = os.path.join(audios_dir, speeches[i])\n",
    "\n",
    "            # ffmpeg command to create video\n",
    "            ffmpeg_command = [\n",
    "                'ffmpeg',\n",
    "                '-loop', '1',\n",
    "                '-i', image_path,\n",
    "                '-i', audio_path,\n",
    "                '-c:v', 'libx264',\n",
    "                '-c:a', 'aac',\n",
    "                '-b:a', '192k',\n",
    "                '-shortest',\n",
    "                os.path.join(output_dir, f'video_{i}.mp4')\n",
    "            ]\n",
    "\n",
    "            # Run ffmpeg command\n",
    "            result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_create('outputs/images', 'outputs/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def zoom_in(video_path, output_path, zoom_factor=1, duration=5):\n",
    "    # Load the video clip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    \n",
    "    # Get the dimensions of the video\n",
    "    width, height = clip.size\n",
    "\n",
    "    # Create the zoom-in effect\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Calculate the position to crop the frame to the original size\n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    # Apply the effect to the clip\n",
    "    zoomed_clip = clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "    # Trim the zoomed clip to the duration\n",
    "    zoomed_clip = zoomed_clip.subclip(0, duration)\n",
    "\n",
    "    # Write the result to a file\n",
    "    zoomed_clip.write_videofile(output_path, codec='libx264')\n",
    "\n",
    "# Example usage\n",
    "zoom_in(\"outputs/final_video/video.mp4\", \"output_video.mp4\", zoom_factor=1.2, duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def zoom_in(video_path, output_path, zoom_factor=1.2):\n",
    "    # Load the video clip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    \n",
    "    # Get the dimensions and duration of the video\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    # Create the zoom-in effect\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Calculate the position to crop the frame to the original size\n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    # Apply the effect to the clip\n",
    "    zoomed_clip = clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "    # Write the result to a file\n",
    "    zoomed_clip.write_videofile(output_path, codec='libx264')\n",
    "\n",
    "# Example usage\n",
    "zoom_in(\"outputs/final_video/video.mp4\", \"output_video.mp4\", zoom_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def left_to_right_pan_with_zoom(video_path, output_path, zoom_factor=1.2):\n",
    "    # Load the video clip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    \n",
    "    # Get the dimensions and duration of the video\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    # Create the left-to-right panning and zooming effect\n",
    "    def pan_and_zoom_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        \n",
    "        # Calculate the current zoom level\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        \n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Calculate the shift amount for horizontal panning\n",
    "        shift = int((new_width - width) * (t / duration))\n",
    "        \n",
    "        # Crop the frame to create the panning effect\n",
    "        x_start = shift\n",
    "        y_start = (new_height - height) // 2\n",
    "        panned_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return panned_frame\n",
    "\n",
    "    # Apply the effect to the clip\n",
    "    panned_zoomed_clip = clip.fl(pan_and_zoom_effect, apply_to=['mask'])\n",
    "\n",
    "    # Write the result to a file\n",
    "    panned_zoomed_clip.write_videofile(output_path, codec='libx264')\n",
    "\n",
    "# Example usage\n",
    "left_to_right_pan_with_zoom(\"outputs/final_video/video.mp4\", \"output_video.mp4\", zoom_factor=1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "\n",
    "def left_to_right_pan(video_path, output_path):\n",
    "    # Load the video clip\n",
    "    clip = VideoFileClip(video_path)\n",
    "    \n",
    "    # Get the dimensions of the video\n",
    "    width, height = clip.size\n",
    "\n",
    "    # Create the left-to-right panning effect\n",
    "    def pan_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        \n",
    "        # Calculate the shift amount for horizontal panning\n",
    "        shift = int(width * t / clip.duration)\n",
    "        \n",
    "        # Shift the frame to the left\n",
    "        shifted_frame = frame[:, shift:shift + width]\n",
    "        \n",
    "        return shifted_frame\n",
    "\n",
    "    # Apply the effect to the clip\n",
    "    panned_clip = clip.fl(pan_effect)\n",
    "\n",
    "    # Write the result to a file\n",
    "    panned_clip.write_videofile(output_path, codec='libx264')\n",
    "\n",
    "# Example usage\n",
    "left_to_right_pan(\"outputs/final_video/video.mp4\", \"output_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
