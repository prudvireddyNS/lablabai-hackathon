{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Agent, Crew, Process\n",
    "from langchain.tools import tool\n",
    "import re\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model='llama3-70b-8192', temperature=1.0, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import SerperDevTool, WebsiteSearchTool, ScrapeWebsiteTool\n",
    "search_tool = SerperDevTool()\n",
    "web_rag_tool = WebsiteSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1)#, doc_content_chars_max=100)\n",
    "# tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class WikiInputs(BaseModel):\n",
    "    \"\"\"Inputs to the wikipedia tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=\"query to look up in Wikipedia, should be 3 or less words\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tool = WikipediaQueryRun(\n",
    "    name=\"wiki-tool\",\n",
    "    description=\"{query:'input here'}\",\n",
    "    args_schema=WikiInputs,\n",
    "    api_wrapper=api_wrapper,\n",
    "    return_direct=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "wiki = Tool(\n",
    "    name = 'wikipedia',\n",
    "    func = wiki_tool.run,\n",
    "    description= \"{query:'input here'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: 2025 elections in India\\nSummary: The 2025 elections in India are expected to include the elections of the Rajya Sabha and 1 state and 1 union territory legislative assemblies.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tool.run(\"latest news in India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def process_script(script):\n",
    "    \"\"\"Used to process the script into dictionary format\"\"\"\n",
    "    dict = {}\n",
    "    dict['text_for_image_generation'] = re.findall(r'<image>(.*?)<image>', script)\n",
    "    dict['text_for_speech_generation'] = re.findall(r'<narration>.*?<narration>', script)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_agent = Agent(\n",
    "    role='Senior Content Writer',\n",
    "    goal='Craft engaging, concise, and informative narrations for YouTube short videos',\n",
    "    backstory=\"\"\"As a seasoned content writer, you excel at breaking down complex topics into captivating narratives that educate and entertain audiences. Your expertise lies in writing short, concise, attention-grabbing scripts for YouTube short videos.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    tools=[wiki_tool]\n",
    ")\n",
    "\n",
    "image_descriptive_agent = Agent(\n",
    "    role='Visual Storyteller',\n",
    "    goal='Design stunning, contextually relevant visuals for YouTube short videos',\n",
    "    backstory='With a keen eye for visual storytelling, you create compelling imagery that elevates the narrative and captivates the audience. You will ',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "ScriptSynthesizer = Agent(\n",
    "    role='Script Processor',\n",
    "    goal='To process scripts by extracting text for image and speech generation',\n",
    "    backstory='you are designed to assist in the automated creation of YouTube Shorts by processing scripts and preparing them for further steps in the content creation pipeline.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False,\n",
    "    tools=[process_script]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_writing_task = Task(\n",
    "    description='Write an engaging narration for a YouTube short video on the topic: {topic}. Use the provided tool if have less idea about the given topic.',\n",
    "    expected_output=\"\"\"A short paragraph suitable for narrating in five seconds also provides immensive experice to audience. Folow the below example for output length and format.\n",
    "\n",
    "    **Example:**\n",
    "\n",
    "    **topic:**\n",
    "    Powerful Kings of History\n",
    "\n",
    "    **narration:**\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations.\n",
    "    From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.\n",
    "    Their leadership continues to inspire awe and fascination to this day.\n",
    "    \"\"\",\n",
    "    agent=script_agent\n",
    ")\n",
    "\n",
    "img_text_task = Task(\n",
    "    description='Given the narration,visually describe each sentence in the narration which will be used as a prompt for an image generation.',\n",
    "    expected_output=\"\"\"Sentences encoded in <narration> and <image> tags. Follow the below example for output format. The output pairs should not be more than five.\n",
    "\n",
    "    example input:\n",
    "    In the pages of history, powerful kings have shaped the destinies of nations. From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations. Their leadership continues to inspire awe and fascination to this day.\n",
    "\n",
    "    outupt format:\n",
    "    <narration>In the pages of history, powerful kings have shaped the destinies of nations.<narration>\n",
    "    <image>An epic portrayal of ancient kings standing triumphantly, clad in regal attire, commanding their kingdoms with strength and wisdom, amidst grandeur and splendor.<image>\n",
    "    <narration>From Alexander the Great to Genghis Khan, their conquests have etched unforgettable legacies across civilizations.<narration>\n",
    "    <image>Dramatic portraits of Alexander the Great and Genghis Khan, adorned in battle armor, leading their armies across vast landscapes and leaving a lasting mark on history.<image>\n",
    "    <narration>Their leadership continues to inspire awe and fascination to this day.<narration>\n",
    "    <image>A powerful visual of kings seated on thrones, symbols of authority and ambition, evoking admiration and wonder, against a backdrop of their enduring achievements.<image>\n",
    "    \"\"\",\n",
    "    agent=image_descriptive_agent,\n",
    "    context=[story_writing_task]\n",
    ")\n",
    "\n",
    "process_script_task = Task(\n",
    "    description=\"Extract text for image and speech generation from a provided script.\",\n",
    "    expected_output=\"A dictionary containing lists of texts for image generation and speech generation.\",\n",
    "    agent=ScriptSynthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-07-03 16:43:53][DEBUG]: == Working Agent: Senior Content Writer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-07-03 16:43:53][INFO]: == Starting Task: Write an engaging narration for a YouTube short video on the topic: AI Agents. Use the provided tool if have less idea about the given topic.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to research and understand the concept of AI Agents to craft an engaging narration for the YouTube short video.\n",
      "\n",
      "Action: wiki-tool\n",
      "Action Input: {\"query\": \"AI Agents\"}\u001b[0m\u001b[95m \n",
      "\n",
      "Page: Intelligent agent\n",
      "Summary: In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner. It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.\n",
      "\n",
      "Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".\n",
      "An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function. For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior, and an evolutionary algorithm's behavior is shaped by a \"fitness function\". \n",
      "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
      "Intelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users).\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-07-03 16:43:58][DEBUG]: == [Senior Content Writer] Task output: Page: Intelligent agent\n",
      "Summary: In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner. It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.\n",
      "\n",
      "Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".\n",
      "An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function. For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior, and an evolutionary algorithm's behavior is shaped by a \"fitness function\". \n",
      "Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n",
      "Intelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users).\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-07-03 16:43:58][DEBUG]: == Working Agent: Visual Storyteller\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-07-03 16:43:58][INFO]: == Starting Task: Given the narration,visually describe each sentence in the narration which will be used as a prompt for an image generation.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can give a great answer.\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "<narration>In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.<narration>\n",
      "<image>A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.<image>\n",
      "\n",
      "<narration>It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.<narration>\n",
      "<image>A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.<image>\n",
      "\n",
      "<narration>An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.<narration>\n",
      "<image>A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.<image>\n",
      "\n",
      "<narration>Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.<narration>\n",
      "<image>A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.<image>\n",
      "\n",
      "<narration>An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.<narration>\n",
      "<image>A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.<image>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-07-03 16:44:00][DEBUG]: == [Visual Storyteller] Task output: <narration>In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.<narration>\n",
      "<image>A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.<image>\n",
      "\n",
      "<narration>It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.<narration>\n",
      "<image>A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.<image>\n",
      "\n",
      "<narration>An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.<narration>\n",
      "<image>A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.<image>\n",
      "\n",
      "<narration>Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.<narration>\n",
      "<image>A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.<image>\n",
      "\n",
      "<narration>An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.<narration>\n",
      "<image>A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.<image>\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-07-03 16:44:00][DEBUG]: == Working Agent: Script Processor\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-07-03 16:44:00][INFO]: == Starting Task: Extract text for image and speech generation from a provided script.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "Formatting errors incremented\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to process the script into a dictionary format.\n",
      "\n",
      "Action: process_script\n",
      "Action Input: {\"script\": \"\"\"<narration>In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.<narration>\n",
      "<image>A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.<image>\n",
      "\n",
      "<narration>It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.<narration>\n",
      "<image>A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.<image>\n",
      "\n",
      "<narration>An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.<narration>\n",
      "<image>A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.<image>\n",
      "\n",
      "<narration>Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.<narration>\n",
      "<image>A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.<image>\n",
      "\n",
      "<narration>An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.<narration>\n",
      "<image>A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.<image>\"\"\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "{'text_for_image_generation': ['A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.', 'A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.', 'A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.', 'A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.', 'A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.'], 'text_for_speech_generation': ['<narration>In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.<narration>', '<narration>It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.<narration>', '<narration>An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.<narration>', '<narration>Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.<narration>', '<narration>An agent has an \"objective function\" that encapsulates all the IA\\'s goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.<narration>']}\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
      "Final Answer: {'text_for_image_generation': ['A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.', 'A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.', 'A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.', 'A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.', 'A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.'], 'text_for_speech_generation': ['In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.', 'It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.', 'An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.', 'Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.', 'An agent has an \"objective function\" that encapsulates all the IA\\'s goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.']}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-07-03 16:44:06][DEBUG]: == [Script Processor] Task output: {'text_for_image_generation': ['A futuristic illustration of a robotic figure with glowing blue circuits, surrounded by swirling patterns of code and data, emphasizing its autonomous and intelligent nature.', 'A split-screen visual of a robotic agent observing its surroundings through a futuristic scope, and a brain-like structure with glowing neurons, symbolizing learning and knowledge acquisition.', 'A collage of diverse examples, including a thermostat, a human silhouette, a cityscape, and a natural landscape, all connected by glowing lines, highlighting the broad range of intelligent agents.', 'A minimalist illustration of a target symbol, with arrows converging on it, surrounded by diagrams and equations, emphasizing goal-directed behavior and intelligent design.', 'A flowchart depicting a complex decision-making process, with arrows and nodes leading to a central goal, surrounded by mathematical formulas and calculators, highlighting the optimization of objective functions.'], 'text_for_speech_generation': ['In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner.', 'It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge.', 'An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.', 'Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence.', 'An agent has an \"objective function\" that encapsulates all the IA\\'s goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.']}\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[script_agent, image_descriptive_agent, ScriptSynthesizer],\n",
    "    tasks=[story_writing_task, img_text_task, process_script_task],\n",
    "    process = Process.sequential,\n",
    "    cache = True,\n",
    "    # memory=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "result = crew.kickoff(inputs={'topic': 'AI Agents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'text_for_image_generation': ['A vibrant illustration of the Indian national flag waving in the wind, with ballot boxes, voting booths, and diverse citizens in the foreground, representing democracy in action, set against a bright and vibrant background symbolizing hope and progress.'], 'text_for_speech_generation': ['The 2025 elections in India are expected to include the elections of the Rajya Sabha and 1 state and 1 union territory legislative assemblies.']}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imagine a world where machines learn to understand human emotions, and relationships are redefined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "filename = os.path.dirname(__name__) + 'outputs/speeches/speech_0.mp3'\n",
    "\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()),\n",
    "      model=\"whisper-large-v3\",\n",
    "      response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video outputs/final_video/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video outputs/final_video/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready outputs/final_video/final_video.mp4\n",
      "Video created at: outputs/final_video/final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "class VideoGeneration(BaseModel):\n",
    "    images_dir: str = Field(description='Path to images directory, such as \"outputs/images\"')\n",
    "    speeches_dir: str = Field(description='Path to speeches directory, such as \"outputs/speeches\"')\n",
    "\n",
    "def add_captions_to_clip(clip, caption, font_path='C:/Users/prudh/Desktop/Python/Generative AI/GenerativeAI-Projects/speed/Montserrat-Bold.ttf', fontsize=40, fontcolor=(255, 255, 255), outline_thickness=2, outline_color=(0, 0, 0)):\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "    width, height = clip.size\n",
    "\n",
    "    def add_caption(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        frame_pil = Image.fromarray(frame)\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "\n",
    "        text_width, text_height = draw.textsize(caption, font=font)\n",
    "        text_x = (width - text_width) // 2\n",
    "        text_y = height - text_height - 50  # 50 pixels from the bottom\n",
    "\n",
    "        for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "            for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                if dx != 0 or dy != 0:\n",
    "                    draw.text((text_x + dx, text_y + dy), caption, font=font, fill=outline_color)\n",
    "        \n",
    "        draw.text((text_x, text_y), caption, font=font, fill=fontcolor)\n",
    "        return np.array(frame_pil)\n",
    "\n",
    "    return clip.fl(lambda gf, t: add_caption(gf, t), apply_to=['mask'])\n",
    "\n",
    "def create_video_from_images_and_audio(images_dir, speeches_dir, zoom_factor=1.2):\n",
    "    \"\"\"Creates video using images and audios.\n",
    "    Args:\n",
    "    images_dir: path to images folder, example 'outputs/images'\n",
    "    speeches_dir: path to speeches folder, example 'outputs/speeches'\"\"\"\n",
    "    images_paths = sorted(os.listdir(images_dir))\n",
    "    audio_paths = sorted(os.listdir(speeches_dir))\n",
    "    clips = []\n",
    "    \n",
    "    for i in range(min(len(images_paths), len(audio_paths))):\n",
    "        # Load the image\n",
    "        img_clip = ImageClip(os.path.join(images_dir, images_paths[i]))\n",
    "        \n",
    "        # Load the audio file\n",
    "        audioclip = AudioFileClip(os.path.join(speeches_dir, audio_paths[i]))\n",
    "        \n",
    "        # Set the duration of the video clip to the duration of the audio file\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        \n",
    "        # Apply zoom-in effect to the video clip\n",
    "        zoomed_clip = apply_zoom_in_effect(videoclip, zoom_factor)\n",
    "        \n",
    "        # Generate transcription for the audio file (caption)\n",
    "        with open(os.path.join(speeches_dir, audio_paths[i]), \"rb\") as file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                file=(audio_paths[i], file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"verbose_json\",\n",
    "            )\n",
    "            caption = transcription.text\n",
    "        \n",
    "        # Add captions to the zoomed video clip\n",
    "        captioned_clip = add_captions_to_clip(zoomed_clip, caption)\n",
    "        \n",
    "        # Add audio to the captioned video clip\n",
    "        captioned_clip = captioned_clip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(captioned_clip)\n",
    "    \n",
    "    # Concatenate all video clips\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    \n",
    "    # Write the result to a file\n",
    "    final_clip.write_videofile(\"outputs/final_video/final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    return \"outputs/final_video/final_video.mp4\"\n",
    "\n",
    "def apply_zoom_in_effect(clip, zoom_factor=1.2):\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Calculate the position to crop the frame to the original size\n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    return clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "# Example usage\n",
    "image_paths = \"outputs/images\"\n",
    "audio_paths = \"outputs/speeches\"\n",
    "\n",
    "video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "print(f\"Video created at: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video temp_zoomed_0.mp4.\n",
      "Moviepy - Writing video temp_zoomed_0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_0.mp4\n",
      "Moviepy - Building video temp_zoomed_1.mp4.\n",
      "Moviepy - Writing video temp_zoomed_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_1.mp4\n",
      "Moviepy - Building video temp_zoomed_2.mp4.\n",
      "Moviepy - Writing video temp_zoomed_2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_2.mp4\n",
      "Moviepy - Building video temp_zoomed_3.mp4.\n",
      "Moviepy - Writing video temp_zoomed_3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_3.mp4\n",
      "Moviepy - Building video outputs/final_video/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video outputs/final_video/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready outputs/final_video/final_video.mp4\n",
      "Video created at: outputs/final_video/final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "class VideoGeneration(BaseModel):\n",
    "    images_dir: str = Field(description='Path to images directory, such as \"outputs/images\"')\n",
    "    speeches_dir: str = Field(description='Path to speeches directory, such as \"outputs/speeches\"')\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def add_text_to_video(input_video, output_video, text, duration=1, fontsize=40, fontcolor=(255, 255, 255),\n",
    "                      outline_thickness=2, outline_color=(0, 0, 0),\n",
    "                      font_path='C:\\\\Users\\\\prudh\\\\Downloads\\\\montserrat\\\\Montserrat\\\\Montserrat-Bold.ttf'):\n",
    "    \n",
    "    chunks = split_text_into_chunks(text, 3)  # Adjust chunk size as needed\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    chunk_duration_frames = duration * fps\n",
    "\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "\n",
    "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        chunk_index = current_frame // chunk_duration_frames\n",
    "\n",
    "        if chunk_index < len(chunks):\n",
    "            chunk = chunks[chunk_index]\n",
    "            text_width, text_height = draw.textsize(chunk, font=font)\n",
    "            text_x = (width - text_width) // 2\n",
    "            text_y = height - 300  # Position text at the bottom\n",
    "\n",
    "            if text_width > width:\n",
    "                words = chunk.split()\n",
    "                half = len(words) // 2\n",
    "                line1 = ' '.join(words[:half])\n",
    "                line2 = ' '.join(words[half:])\n",
    "\n",
    "                text_size_line1 = draw.textsize(line1, font=font)\n",
    "                text_size_line2 = draw.textsize(line2, font=font)\n",
    "                text_x_line1 = (width - text_size_line1[0]) // 2\n",
    "                text_x_line2 = (width - text_size_line2[0]) // 2\n",
    "                text_y = height - 250 - text_size_line1[1]  # Adjust vertical position for two lines\n",
    "\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x_line1 + dx, text_y + dy), line1, font=font, fill=outline_color)\n",
    "                            draw.text((text_x_line2 + dx, text_y + text_size_line1[1] + dy), line2, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x_line1, text_y), line1, font=font, fill=fontcolor)\n",
    "                draw.text((text_x_line2, text_y + text_size_line1[1]), line2, font=font, fill=fontcolor)\n",
    "\n",
    "            else:\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x + dx, text_y + dy), chunk, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x, text_y), chunk, font=font, fill=fontcolor)\n",
    "\n",
    "            frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def apply_zoom_in_effect(clip, zoom_factor=1.2):\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    return clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "def create_video_from_images_and_audio(images_dir, speeches_dir, zoom_factor=1.2):\n",
    "    images_paths = sorted(os.listdir(images_dir))\n",
    "    audio_paths = sorted(os.listdir(speeches_dir))\n",
    "    clips = []\n",
    "    \n",
    "    for i in range(min(len(images_paths), len(audio_paths))):\n",
    "        img_clip = ImageClip(os.path.join(images_dir, images_paths[i]))\n",
    "        audioclip = AudioFileClip(os.path.join(speeches_dir, audio_paths[i]))\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        zoomed_clip = apply_zoom_in_effect(videoclip, zoom_factor)\n",
    "        \n",
    "        with open(os.path.join(speeches_dir, audio_paths[i]), \"rb\") as file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                file=(audio_paths[i], file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"verbose_json\",\n",
    "            )\n",
    "            caption = transcription.text\n",
    "        \n",
    "        temp_video_path = f\"temp_zoomed_{i}.mp4\"\n",
    "        zoomed_clip.write_videofile(temp_video_path, codec='libx264', fps=24)\n",
    "        \n",
    "        final_video_path = f\"temp_captioned_{i}.mp4\"\n",
    "        add_text_to_video(temp_video_path, final_video_path, caption, duration=1)\n",
    "        final_clip = VideoFileClip(final_video_path)\n",
    "        final_clip = final_clip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(final_clip)\n",
    "    \n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(\"outputs/final_video/final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    return \"outputs/final_video/final_video.mp4\"\n",
    "\n",
    "# Example usage\n",
    "image_paths = \"outputs/images\"\n",
    "audio_paths = \"outputs/speeches\"\n",
    "\n",
    "video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "print(f\"Video created at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video temp_zoomed_0.mp4.\n",
      "Moviepy - Writing video temp_zoomed_0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_0.mp4\n",
      "Moviepy - Building video temp_zoomed_1.mp4.\n",
      "Moviepy - Writing video temp_zoomed_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_1.mp4\n",
      "Moviepy - Building video temp_zoomed_2.mp4.\n",
      "Moviepy - Writing video temp_zoomed_2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_2.mp4\n",
      "Moviepy - Building video temp_zoomed_3.mp4.\n",
      "Moviepy - Writing video temp_zoomed_3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_3.mp4\n",
      "Moviepy - Building video outputs/final_video/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video outputs/final_video/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready outputs/final_video/final_video.mp4\n",
      "Video created at: outputs/final_video/final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips, VideoFileClip\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "# class VideoGeneration(BaseModel):\n",
    "#     images_dir: str = Field(description='Path to images directory, such as \"outputs/images\"')\n",
    "#     speeches_dir: str = Field(description='Path to speeches directory, such as \"outputs/speeches\"')\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def add_text_to_video(input_video, output_video, text, duration=1, fontsize=40, fontcolor=(255, 255, 255),\n",
    "                      outline_thickness=2, outline_color=(0, 0, 0),\n",
    "                      font_path='C:\\\\Users\\\\prudh\\\\Downloads\\\\montserrat\\\\Montserrat\\\\Montserrat-Bold.ttf'):\n",
    "    \n",
    "    chunks = split_text_into_chunks(text, 3)  # Adjust chunk size as needed\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    chunk_duration_frames = duration * fps\n",
    "\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "\n",
    "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        chunk_index = current_frame // chunk_duration_frames\n",
    "\n",
    "        if chunk_index < len(chunks):\n",
    "            chunk = chunks[chunk_index]\n",
    "            text_width, text_height = draw.textsize(chunk, font=font)\n",
    "            text_x = (width - text_width) // 2\n",
    "            text_y = height - 300  # Position text at the bottom\n",
    "\n",
    "            if text_width > width:\n",
    "                words = chunk.split()\n",
    "                half = len(words) // 2\n",
    "                line1 = ' '.join(words[:half])\n",
    "                line2 = ' '.join(words[half:])\n",
    "\n",
    "                text_size_line1 = draw.textsize(line1, font=font)\n",
    "                text_size_line2 = draw.textsize(line2, font=font)\n",
    "                text_x_line1 = (width - text_size_line1[0]) // 2\n",
    "                text_x_line2 = (width - text_size_line2[0]) // 2\n",
    "                text_y = height - 250 - text_size_line1[1]  # Adjust vertical position for two lines\n",
    "\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x_line1 + dx, text_y + dy), line1, font=font, fill=outline_color)\n",
    "                            draw.text((text_x_line2 + dx, text_y + text_size_line1[1] + dy), line2, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x_line1, text_y), line1, font=font, fill=fontcolor)\n",
    "                draw.text((text_x_line2, text_y + text_size_line1[1]), line2, font=font, fill=fontcolor)\n",
    "\n",
    "            else:\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x + dx, text_y + dy), chunk, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x, text_y), chunk, font=font, fill=fontcolor)\n",
    "\n",
    "            frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def apply_zoom_in_effect(clip, zoom_factor=1.2):\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    return clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "def create_video_from_images_and_audio(images_dir, speeches_dir, zoom_factor=1.2):\n",
    "    images_paths = sorted(os.listdir(images_dir))\n",
    "    audio_paths = sorted(os.listdir(speeches_dir))\n",
    "    clips = []\n",
    "    temp_files = []\n",
    "    \n",
    "    for i in range(min(len(images_paths), len(audio_paths))):\n",
    "        img_clip = ImageClip(os.path.join(images_dir, images_paths[i]))\n",
    "        audioclip = AudioFileClip(os.path.join(speeches_dir, audio_paths[i]))\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        zoomed_clip = apply_zoom_in_effect(videoclip, zoom_factor)\n",
    "        \n",
    "        with open(os.path.join(speeches_dir, audio_paths[i]), \"rb\") as file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                file=(audio_paths[i], file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"verbose_json\",\n",
    "            )\n",
    "            caption = transcription.text\n",
    "        \n",
    "        temp_video_path = f\"temp_zoomed_{i}.mp4\"\n",
    "        zoomed_clip.write_videofile(temp_video_path, codec='libx264', fps=24)\n",
    "        temp_files.append(temp_video_path)\n",
    "        \n",
    "        final_video_path = f\"temp_captioned_{i}.mp4\"\n",
    "        add_text_to_video(temp_video_path, final_video_path, caption, duration=1)\n",
    "        temp_files.append(final_video_path)\n",
    "        \n",
    "        final_clip = VideoFileClip(final_video_path)\n",
    "        final_clip = final_clip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(final_clip)\n",
    "    \n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(\"outputs/final_video/final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    # Close all video files properly\n",
    "    for clip in clips:\n",
    "        clip.close()\n",
    "        \n",
    "    # Remove all temporary files\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            os.remove(temp_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing file {temp_file}: {e}\")\n",
    "    \n",
    "    return \"outputs/final_video/final_video.mp4\"\n",
    "\n",
    "# Example usage\n",
    "image_paths = \"outputs/images\"\n",
    "audio_paths = \"outputs/speeches\"\n",
    "\n",
    "video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "print(f\"Video created at: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video temp_zoomed_0.mp4.\n",
      "Moviepy - Writing video temp_zoomed_0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_0.mp4\n",
      "Moviepy - Building video temp_zoomed_1.mp4.\n",
      "Moviepy - Writing video temp_zoomed_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_1.mp4\n",
      "Moviepy - Building video temp_zoomed_2.mp4.\n",
      "Moviepy - Writing video temp_zoomed_2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_2.mp4\n",
      "Moviepy - Building video temp_zoomed_3.mp4.\n",
      "Moviepy - Writing video temp_zoomed_3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp_zoomed_3.mp4\n",
      "Moviepy - Building video outputs/final_video/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video outputs/final_video/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready outputs/final_video/final_video.mp4\n",
      "Video created at: outputs/final_video/final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips, VideoFileClip\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "# class VideoGeneration(BaseModel):\n",
    "#     images_dir: str = Field(description='Path to images directory, such as \"outputs/images\"')\n",
    "#     speeches_dir: str = Field(description='Path to speeches directory, such as \"outputs/speeches\"')\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def add_text_to_video(input_video, output_video, text, duration=1, fontsize=40, fontcolor=(255, 255, 255),\n",
    "                      outline_thickness=2, outline_color=(0, 0, 0), delay_between_chunks=0.2,\n",
    "                      font_path='C:\\\\Users\\\\prudh\\\\Downloads\\\\montserrat\\\\Montserrat\\\\Montserrat-Bold.ttf'):\n",
    "    \n",
    "    chunks = split_text_into_chunks(text, 3)  # Adjust chunk size as needed\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    chunk_duration_frames = duration * fps\n",
    "    delay_frames = int(delay_between_chunks * fps)\n",
    "\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "\n",
    "    current_frame = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "\n",
    "        chunk_index = current_frame // (chunk_duration_frames + delay_frames)\n",
    "\n",
    "        if current_frame % (chunk_duration_frames + delay_frames) < chunk_duration_frames and chunk_index < len(chunks):\n",
    "            chunk = chunks[chunk_index]\n",
    "            text_width, text_height = draw.textsize(chunk, font=font)\n",
    "            text_x = (width - text_width) // 2\n",
    "            text_y = height - 300  # Position text at the bottom\n",
    "\n",
    "            if text_width > width:\n",
    "                words = chunk.split()\n",
    "                half = len(words) // 2\n",
    "                line1 = ' '.join(words[:half])\n",
    "                line2 = ' '.join(words[half:])\n",
    "\n",
    "                text_size_line1 = draw.textsize(line1, font=font)\n",
    "                text_size_line2 = draw.textsize(line2, font=font)\n",
    "                text_x_line1 = (width - text_size_line1[0]) // 2\n",
    "                text_x_line2 = (width - text_size_line2[0]) // 2\n",
    "                text_y = height - 250 - text_size_line1[1]  # Adjust vertical position for two lines\n",
    "\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x_line1 + dx, text_y + dy), line1, font=font, fill=outline_color)\n",
    "                            draw.text((text_x_line2 + dx, text_y + text_size_line1[1] + dy), line2, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x_line1, text_y), line1, font=font, fill=fontcolor)\n",
    "                draw.text((text_x_line2, text_y + text_size_line1[1]), line2, font=font, fill=fontcolor)\n",
    "\n",
    "            else:\n",
    "                for dx in range(-outline_thickness, outline_thickness + 1):\n",
    "                    for dy in range(-outline_thickness, outline_thickness + 1):\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((text_x + dx, text_y + dy), chunk, font=font, fill=outline_color)\n",
    "                \n",
    "                draw.text((text_x, text_y), chunk, font=font, fill=fontcolor)\n",
    "\n",
    "            frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        out.write(frame)\n",
    "        current_frame += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def apply_zoom_in_effect(clip, zoom_factor=1.2):\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    return clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "def create_video_from_images_and_audio(images_dir, speeches_dir, zoom_factor=1.2):\n",
    "    images_paths = sorted(os.listdir(images_dir))\n",
    "    audio_paths = sorted(os.listdir(speeches_dir))\n",
    "    clips = []\n",
    "    temp_files = []\n",
    "    \n",
    "    for i in range(min(len(images_paths), len(audio_paths))):\n",
    "        img_clip = ImageClip(os.path.join(images_dir, images_paths[i]))\n",
    "        audioclip = AudioFileClip(os.path.join(speeches_dir, audio_paths[i]))\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        zoomed_clip = apply_zoom_in_effect(videoclip, zoom_factor)\n",
    "        \n",
    "        with open(os.path.join(speeches_dir, audio_paths[i]), \"rb\") as file:\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                file=(audio_paths[i], file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                response_format=\"verbose_json\",\n",
    "            )\n",
    "            caption = transcription.text\n",
    "        \n",
    "        temp_video_path = f\"temp_zoomed_{i}.mp4\"\n",
    "        zoomed_clip.write_videofile(temp_video_path, codec='libx264', fps=24)\n",
    "        temp_files.append(temp_video_path)\n",
    "        \n",
    "        final_video_path = f\"temp_captioned_{i}.mp4\"\n",
    "        add_text_to_video(temp_video_path, final_video_path, caption, duration=1)\n",
    "        temp_files.append(final_video_path)\n",
    "        \n",
    "        final_clip = VideoFileClip(final_video_path)\n",
    "        final_clip = final_clip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(final_clip)\n",
    "    \n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(\"outputs/final_video/final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    # Close all video files properly\n",
    "    for clip in clips:\n",
    "        clip.close()\n",
    "        \n",
    "    # Remove all temporary files\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            os.remove(temp_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing file {temp_file}: {e}\")\n",
    "    \n",
    "    return \"outputs/final_video/final_video.mp4\"\n",
    "\n",
    "# Example usage\n",
    "image_paths = \"outputs/images\"\n",
    "audio_paths = \"outputs/speeches\"\n",
    "\n",
    "video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "print(f\"Video created at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
