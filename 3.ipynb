{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Task, Agent, Crew, Process\n",
    "from langchain.tools import tool\n",
    "import re\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model='llama3-70b-8192', temperature=0.6, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hi').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prudh\\anaconda3\\envs\\gen_ai1\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9be546a5c374d18b541d78db393f8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, StableDiffusionXLPipeline, DPMSolverSinglestepScheduler\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pyttsx3\n",
    "import os\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"sd-community/sdxl-flash\", torch_dtype=torch.float16).to('cuda')\n",
    "pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "def quantize_model_to_4bit(model):\n",
    "    replacements = []\n",
    "\n",
    "    # Collect layers to be replaced\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            replacements.append((name, module))\n",
    "\n",
    "    # Replace layers\n",
    "    for name, module in replacements:\n",
    "        # Split the name to navigate to the parent module\n",
    "        *path, last = name.split('.')\n",
    "        parent = model\n",
    "        for part in path:\n",
    "            parent = getattr(parent, part)\n",
    "\n",
    "        # Create and assign the quantized layer\n",
    "        quantized_layer = bnb.nn.Linear4bit(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "        quantized_layer.weight.data = module.weight.data\n",
    "        if module.bias is not None:\n",
    "            quantized_layer.bias.data = module.bias.data\n",
    "        setattr(parent, last, quantized_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "pipe.unet = quantize_model_to_4bit(pipe.unet)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "def generate_speech(text, speech_dir='./outputs/audio', lang='en', speed=170, voice='default', num=0):\n",
    "    \"\"\"\n",
    "    Generates speech for given script.\n",
    "    \"\"\"\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Set language and voice\n",
    "    voices = engine.getProperty('voices')\n",
    "    if voice == 'default':\n",
    "        voice_id = voices[1].id\n",
    "    else:\n",
    "        # Try to find the voice with the given name\n",
    "        voice_id = None\n",
    "        for v in voices:\n",
    "            if voice in v.name:\n",
    "                voice_id = v.id\n",
    "                break\n",
    "        if not voice_id:\n",
    "            raise ValueError(f\"Voice '{voice}' not found.\")\n",
    "    \n",
    "    engine.setProperty('voice', voice_id)\n",
    "    engine.setProperty('rate', speed)\n",
    "    os.remove(os.path.join(speech_dir, f'speech_{num}.mp3')) if os.path.exists(os.path.join(speech_dir, f'speech_{num}.mp3')) else None\n",
    "    engine.save_to_file(text, os.path.join(speech_dir, f'speech_{num}.mp3'))\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class VideoGeneration(BaseModel):\n",
    "    images_dir : str = Field(description='Path to images directory, such as \"outputs/images\"')\n",
    "    speeches_dir : str = Field(description='Path to speeches directory, such as \"outputs/speeches\"')\n",
    "\n",
    "@tool(args_schema=VideoGeneration)\n",
    "def create_video_from_images_and_audio(images_dir, speeches_dir, zoom_factor=1.2):\n",
    "    \"\"\"Creates video using images and audios with zoom-in effect\"\"\"\n",
    "    images_paths = os.listdir(images_dir)\n",
    "    audio_paths = os.listdir(speeches_dir)\n",
    "    # print(images_paths, audio_paths)\n",
    "    clips = []\n",
    "    \n",
    "    for i in range(min(len(images_paths), len(audio_paths))):\n",
    "        # Load the image\n",
    "        img_clip = ImageClip(os.path.join(images_dir, images_paths[i]))\n",
    "        \n",
    "        # Load the audio file\n",
    "        audioclip = AudioFileClip(os.path.join(speeches_dir, audio_paths[i]))\n",
    "        \n",
    "        # Set the duration of the video clip to the duration of the audio file\n",
    "        videoclip = img_clip.set_duration(audioclip.duration)\n",
    "        \n",
    "        # Apply zoom-in effect to the video clip\n",
    "        zoomed_clip = apply_zoom_in_effect(videoclip, zoom_factor)\n",
    "        \n",
    "        # Add audio to the zoomed video clip\n",
    "        zoomed_clip = zoomed_clip.set_audio(audioclip)\n",
    "        \n",
    "        clips.append(zoomed_clip)\n",
    "    \n",
    "    # Concatenate all video clips\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    \n",
    "    # Write the result to a file\n",
    "    final_clip.write_videofile(\"outputs/final_video/final_video.mp4\", codec='libx264', fps=24)\n",
    "    \n",
    "    return \"final_video.mp4\"\n",
    "\n",
    "def apply_zoom_in_effect(clip, zoom_factor=1.2):\n",
    "    width, height = clip.size\n",
    "    duration = clip.duration\n",
    "\n",
    "    def zoom_in_effect(get_frame, t):\n",
    "        frame = get_frame(t)\n",
    "        zoom = 1 + (zoom_factor - 1) * (t / duration)\n",
    "        new_width, new_height = int(width * zoom), int(height * zoom)\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        \n",
    "        # Calculate the position to crop the frame to the original size\n",
    "        x_start = (new_width - width) // 2\n",
    "        y_start = (new_height - height) // 2\n",
    "        cropped_frame = resized_frame[y_start:y_start + height, x_start:x_start + width]\n",
    "        \n",
    "        return cropped_frame\n",
    "\n",
    "    return clip.fl(zoom_in_effect, apply_to=['mask'])\n",
    "\n",
    "# Example usage\n",
    "# image_paths = \"outputs/images\"\n",
    "# audio_paths = \"outputs/audio\"\n",
    "\n",
    "# video_path = create_video_from_images_and_audio(image_paths, audio_paths)\n",
    "# print(f\"Video created at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGeneration(BaseModel):\n",
    "    text : str = Field(description='description of sentence used for image generation')\n",
    "    num : int = Field(description='sequence of description passed this tool. Used in image saving path.')\n",
    "\n",
    "class SpeechGeneration(BaseModel):\n",
    "    text : str = Field(description='description of sentence used for image generation')\n",
    "    num : int = Field(description='sequence of description passed this tool. Used in image saving path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=ImageGeneration)\n",
    "def image_generator(text,num):\n",
    "    \"\"\"Generates images for the given narration.\n",
    "    Saves it to images_dir and return path\"\"\"\n",
    "    images_dir = './outputs/images'\n",
    "    if num==1:\n",
    "        for filename in os.listdir(images_dir):\n",
    "            file_path = os.path.join(images_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    \n",
    "    if '<image>' in text:\n",
    "        text = re.findall(r'<image>(.*?)<image>', text)\n",
    "    else:\n",
    "        text = text\n",
    "    image = pipe(text, num_inference_steps=6, guidance_scale=2, width=720, height=1280, verbose=0).images[0]\n",
    "    # print(num)\n",
    "    os.remove(os.path.join(images_dir, f'image{num}.jpg')) if os.path.exists(os.path.join(images_dir, f'image{num}.jpg')) else None\n",
    "    image.save(os.path.join(images_dir, f'image{num}.jpg'))\n",
    "    return f'image {num} generated.'#f'image generated for \"{text}\" and saved to directory {images_dir} as image{num}.jpg'\n",
    "\n",
    "@tool\n",
    "def speech_generator(text, num):\n",
    "    \"\"\"Generates speech for given text\n",
    "    Saves it to speech_dir and return path\"\"\"\n",
    "    speech_dir = './outputs/speeches'\n",
    "\n",
    "    if num==1:\n",
    "        for filename in os.listdir(speech_dir):\n",
    "            file_path = os.path.join(speech_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    \n",
    "    if '<narration>' in text:\n",
    "        text = re.findall(r'<narration>(.*?)<narration>', text)\n",
    "    else:\n",
    "        text = text\n",
    "    # print(num)\n",
    "    generate_speech(text, speech_dir, num=num)\n",
    "    return f'speech {num} generated.'#f'speech generated for \"{text}\" and saved to directory {speech_dir} as speech{num}.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_agent = Agent(\n",
    "    role='Senior Content Writer',\n",
    "    goal='Craft engaging, concise, and informative narrations for YouTube short videos',\n",
    "    backstory=\"\"\"As a seasoned content writer, you excel at breaking down complex topics into captivating narratives that educate and entertain audiences. Your expertise lies in writing concise, attention-grabbing scripts for YouTube short videos.\"\"\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "image_descriptive_agent = Agent(\n",
    "    role='Visual Storyteller',\n",
    "    goal='Design stunning, contextually relevant visuals for YouTube short videos',\n",
    "    backstory='With a keen eye for visual storytelling, you create compelling imagery that elevates the narrative and captivates the audience. You will ',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "img_speech_generating_agent = Agent(\n",
    "    role='Multimedia Content Creator',\n",
    "    goal='Generate high-quality images and speeches for YouTube short videos one after another based on provided descriptions.',\n",
    "    backstory='As a multimedia expert, you excel at creating engaging multimedia content that brings stories to life.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    role = 'Video editor',\n",
    "    goal = 'To make a video for YouTube shorts.',\n",
    "    backstory = \"You are a video editor working for a YouTube creator\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    allow_delegation = False,\n",
    "    tools = [create_video_from_images_and_audio]\n",
    ")\n",
    "\n",
    "story_writing_task = Task(\n",
    "    description='Write an engaging narration for a YouTube short video on the topic: {topic}',\n",
    "    expected_output=\"\"\"A short paragraph suitable for narrating in five seconds that provides an immersive experience to the audience. Follow the below example for output length and format.\n",
    "\n",
    "    **Example input:**\n",
    "    Ancient Wonders of the World\n",
    "\n",
    "    **Output format:**\n",
    "    Discover the ancient wonders! From the Great Pyramid to the Hanging Gardens, these marvels still captivate our imaginations today.\n",
    "    \"\"\",\n",
    "    agent=script_agent\n",
    ")\n",
    "\n",
    "\n",
    "img_text_task = Task(\n",
    "    description='Given the narration, visually describe each sentence in the narration which will be used as a prompt for image generation.',\n",
    "    expected_output=\"\"\"Sentences encoded in <narration> and <image> tags. Follow the below example for output format.\n",
    "\n",
    "    **Example input:**\n",
    "    Discover the ancient wonders! From the Great Pyramid to the Hanging Gardens, these marvels still captivate our imaginations today.\n",
    "\n",
    "    **Output format:**\n",
    "    <narration>1. Discover the ancient wonders!<narration>\n",
    "    <image>1. A stunning view of various ancient wonders, showcasing their grandeur and mystery.<image>\n",
    "\n",
    "    <narration>2. From the Great Pyramid to the Hanging Gardens,<narration>\n",
    "    <image>2. The Great Pyramid of Giza and the lush Hanging Gardens depicted in their full glory.<image>\n",
    "    \n",
    "    <narration>3. these marvels still captivate our imaginations today.<narration>\n",
    "    <image>3. People today gazing at replicas or illustrations of these ancient wonders, filled with awe.<image>\n",
    "    \"\"\",\n",
    "    agent=image_descriptive_agent,\n",
    "    context=[story_writing_task]\n",
    ")\n",
    "\n",
    "\n",
    "# process_script_task = Task(\n",
    "#     description=\"Extract text for image and speech generation from a provided script.\",\n",
    "#     expected_output=\"A dictionary containing lists of texts for image generation and speech generation.\",\n",
    "#     agent=ScriptSynthesizer\n",
    "# )\n",
    "\n",
    "img_generation_task = Task(\n",
    "    description='Given the input generate images for each sentence enclosed in <image> tag.',\n",
    "    expected_output=\"\"\"Acknowledgement of image generation\"\"\",\n",
    "    tools = [image_generator],\n",
    "    context = [img_text_task],\n",
    "    # async_execution=True,\n",
    "    agent=img_speech_generating_agent\n",
    ")\n",
    "\n",
    "speech_generation_task = Task(\n",
    "    description='Given the input generate speech for each sentence enclosed in <narration> tag.',\n",
    "    expected_output=\"\"\"Acknowledgement of speech generation\"\"\",\n",
    "    tools = [speech_generator],\n",
    "    context = [img_text_task],\n",
    "    # async_execution=True,\n",
    "    agent=img_speech_generating_agent\n",
    ")\n",
    "\n",
    "make_video_task = Task(\n",
    "    description = 'Create video using images and speeches from the forlders \"outpus/images\" and \"outputs/speeches\"',\n",
    "    expected_output = \"output video path\",\n",
    "    agent=editor,\n",
    "    context = [img_generation_task, speech_generation_task]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 15:00:59,523 - 3164 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[script_agent, image_descriptive_agent, img_speech_generating_agent, editor],\n",
    "    tasks=[story_writing_task, img_text_task, img_generation_task,speech_generation_task,make_video_task],\n",
    "    process = Process.sequential,\n",
    "    # cache = True,\n",
    "    # memory=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# result = crew.kickoff(inputs={'topic': 'Italy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"final_video.mp4\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prudh\\\\Desktop\\\\Python\\\\Generative AI\\\\GenerativeAI-Projects\\\\10sec Shorts'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.abspath(__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mLet's start generating images for each sentence enclosed in the <image> tag.\n",
      "\n",
      "Thought: I need to generate high-quality images for each sentence enclosed in the <image> tag.\n",
      "\n",
      "Action: image_generator\n",
      "Action Input: {\"text\": \"A dreamy, soft-focused shot of a person walking through a bustling French market, surrounded by vibrant flowers, pastel-colored buildings, and a subtle Eiffel Tower silhouette in the background, evoking a sense of romance and whimsy.\", \"num\": 1}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_video(topic):\n",
    "    crew.kickoff(inputs={'topic': topic})\n",
    "    return os.path.join(os.path.dirname(os.path.abspath(__name__)), 'outputs/final_video/final_video.mp4')\n",
    "\n",
    "app = gr.Interface(\n",
    "    fn=generate_video,\n",
    "    inputs='text',\n",
    "    # outputs=gr.Video(value=os.path.join('outputs/final_video/video.mp4'),label=\"Generated Video\", width=720/2, height=1280/2),\n",
    "    outputs = gr.Video(format='mp4',label=\"Generated Video\", width=720/2, height=1280/2),\n",
    "    title=\"ShortsIn\",\n",
    "    description=\"Shorts generator\"\n",
    ")\n",
    "\n",
    "app.launch()\n",
    "#os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
